{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras import Sequential, Model\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
    "import copy\n",
    "\n",
    "import Environment as E\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "\n",
    "n_agents = 1\n",
    "\n",
    "# Model\n",
    "\n",
    "time_steps = 3\n",
    "grid_size_w = 20\n",
    "grid_size_h = 20\n",
    "n_actions = 5\n",
    "\n",
    "n_channels = time_steps + time_steps + 1\n",
    "\n",
    "# Q-Learning\n",
    "\n",
    "n_episodes = 10\n",
    "n_steps = 1000000\n",
    "epsilon = 0.75\n",
    "epsilon_decay = 0.99\n",
    "gamma = 0.9\n",
    "\n",
    "# Policy Gradient\n",
    "\n",
    "n_runs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_i = Input(shape=(time_steps, grid_size_w, grid_size_h))\n",
    "conv_1 = Conv2D(16, (4, 4), activation=\"relu\", data_format=\"channels_first\")(data_i)\n",
    "conv_2 = Conv2D(32, (2, 2), activation=\"relu\", data_format=\"channels_first\")(conv_1)\n",
    "pool_1 = MaxPooling2D(data_format=\"channels_first\")(conv_2)\n",
    "drop_1 = Dropout(0.25)(pool_1)\n",
    "flat_1 = Flatten()(drop_1)\n",
    "feed_1 = Dense(64, activation=\"relu\")(flat_1)\n",
    "drop_2 = Dropout(0.25)(feed_1)\n",
    "feed_2 = Dense(n_actions, activation=\"linear\")(drop_2)\n",
    "data_o = feed_2\n",
    "\n",
    "K.print_tensor(data_o)\n",
    "\n",
    "model = Model(inputs=data_i, outputs=data_o)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuple_to_batch(known_data):\n",
    "    batch = []\n",
    "    for i,agent_pos in enumerate(known_data[2]):\n",
    "        batch.append(np.concatenate((known_data[0],known_data[1],known_data[2][i:i+1])))\n",
    "    batch = np.array(batch)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "importlib.reload(E)\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    env = E.Environment((20,20))\n",
    "    d = []\n",
    "    action_avg = np.zeros((400,5))\n",
    "    list_avg_rwd = []\n",
    "    list_steps = []\n",
    "    for k in range(n_agents):\n",
    "        env.add_agent(coord=(k,0),capacity=100000)\n",
    "    \n",
    "    X = env.export_known_data()\n",
    "    X = tuple_to_batch(X)\n",
    "    Q_vals = model.predict(X)\n",
    "    if i % 10 == 0:\n",
    "        print(\"Episode {} of {}\".format(i + 1, n_episodes))\n",
    "    \n",
    "    r_sum = 0\n",
    "    \n",
    "    for step in range(n_steps):\n",
    "        if np.random.random() < epsilon:\n",
    "            actions = np.random.randint(0, high=n_actions, size=n_agents).tolist()\n",
    "        else:\n",
    "            actions = [np.argmax(q_array) for q_array in Q_vals]\n",
    "        \n",
    "        X_reward = env.move_agents(actions) #new known state and rewards\n",
    "        # after all agents move\n",
    "        X_new = tuple_to_batch(X_reward[:3])\n",
    "        rewards = X_reward[3]\n",
    "        action_avg[step%400, : ] = 0\n",
    "        action_avg[step%400,actions[0]]= 1\n",
    "        d.append(rewards[0])\n",
    "        while(len(d)>400):\n",
    "            del(d[0])\n",
    "        if(step% 1000 == 0): \n",
    "            mean = sum(d) / 400.0\n",
    "            epsilon *= epsilon_decay\n",
    "            print(epsilon)\n",
    "            print(\"In Step {} the average reward of 100 is {} \".format(step, mean))\n",
    "            print(\"Actions: {}\".format(np.sum(action_avg, axis = 0)))\n",
    "            list_avg_rwd.append(mean)\n",
    "            list_steps.append(step)\n",
    "            plt.scatter(list_steps, list_avg_rwd)\n",
    "            plt.show()\n",
    "        Q_vals_new = model.predict(X_new)\n",
    "        \n",
    "        Q_vals[:, actions] = rewards + gamma * np.max(Q_vals_new, axis=1)\n",
    "        model.fit(X, Q_vals, epochs=1, verbose=0)\n",
    "        \n",
    "        Q_vals = Q_vals_new\n",
    "        X = X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Trashbots)",
   "language": "python",
   "name": "trashbots"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
