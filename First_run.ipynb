{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras import Sequential, Model\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
    "import copy\n",
    "\n",
    "import Environment as E\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "\n",
    "n_agents = 1\n",
    "\n",
    "# Model\n",
    "\n",
    "time_steps = 3\n",
    "grid_size_w = 7\n",
    "grid_size_h = 7\n",
    "n_actions = 5\n",
    "\n",
    "n_channels = time_steps + time_steps + 1\n",
    "\n",
    "# Q-Learning\n",
    "\n",
    "n_episodes = 10\n",
    "n_steps = 1000000\n",
    "epsilon = 0.75\n",
    "epsilon_decay = 0.999\n",
    "gamma = 0.9\n",
    "\n",
    "# Policy Gradient\n",
    "\n",
    "n_runs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_i = Input(shape=(time_steps, grid_size_w, grid_size_h))\n",
    "conv_1 = Conv2D(16, (2, 2), activation=\"relu\", data_format=\"channels_first\")(data_i)\n",
    "conv_2 = Conv2D(32, (2, 2), activation=\"relu\", data_format=\"channels_first\")(conv_1)\n",
    "pool_1 = MaxPooling2D(data_format=\"channels_first\")(conv_2)\n",
    "drop_1 = Dropout(0.25)(pool_1)\n",
    "flat_1 = Flatten()(drop_1)\n",
    "feed_1 = Dense(64, activation=\"relu\")(flat_1)\n",
    "drop_2 = Dropout(0.25)(feed_1)\n",
    "feed_2 = Dense(n_actions, activation=\"linear\")(drop_2)\n",
    "data_o = feed_2\n",
    "\n",
    "K.print_tensor(data_o)\n",
    "\n",
    "model = Model(inputs=data_i, outputs=data_o)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuple_to_batch(known_data):\n",
    "    batch = []\n",
    "    for i,agent_pos in enumerate(known_data[2]):\n",
    "        batch.append(np.concatenate((known_data[0],known_data[1],known_data[2][i:i+1])))\n",
    "    batch = np.array(batch)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "importlib.reload(E)\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    env = E.Environment(dim = (grid_size_h,grid_size_w),reward_eat_trash=10, \n",
    "                        reward_invalid_move=0, reward_nothing_happend=0, \n",
    "                        trash_appearence_prob=0.1, number_trash_sources=1, saved_timesteps=time_steps )\n",
    "    d = []\n",
    "    action_avg = np.zeros((10000,5))\n",
    "    list_avg_rwd = []\n",
    "    list_steps = []\n",
    "    all_time_reward = 0\n",
    "    all_time_reward_avg_history = []\n",
    "    all_time_reward_steps_history = []\n",
    "    for k in range(n_agents):\n",
    "        env.add_agent(coord=(k,0),capacity=100000)\n",
    "    \n",
    "    X = env.export_known_data()\n",
    "    X = tuple_to_batch(X)\n",
    "    Q_vals = model.predict(X)\n",
    "    if i % 10 == 0:\n",
    "        print(\"Episode {} of {}\".format(i + 1, n_episodes))\n",
    "    \n",
    "    r_sum = 0\n",
    "    \n",
    "    for step in range(n_steps):\n",
    "        if np.random.random() < max(epsilon, 0.01):\n",
    "            actions = np.random.randint(0, high=n_actions, size=n_agents).tolist()\n",
    "        else:\n",
    "            actions = np.apply_along_axis(np.argmax, 1, Q_vals).tolist()\n",
    "        #print(\"Actions {}\".format(actions))\n",
    "        X_reward = env.move_agents(actions) #new known state and rewards\n",
    "        # after all agents move\n",
    "        X_new = tuple_to_batch(X_reward[:3])\n",
    "        rewards = X_reward[3]\n",
    "        #print(\"Rewards {}\".format(rewards))\n",
    "        all_time_reward += sum(rewards)\n",
    "        action_avg[step%10000, : ] = 0\n",
    "        action_avg[step%10000,actions[0]]= 1\n",
    "        d.append(sum(rewards))\n",
    "        while(len(d)>10000):\n",
    "            del(d[0])\n",
    "        if(step% 10000 == 0): \n",
    "            mean = sum(d) / 10000.0\n",
    "            \n",
    "            epsilon *= epsilon_decay\n",
    "            print(\"Current Random level is {}\".format(max(epsilon, 0.01)))\n",
    "            print(\"In Step {} the average reward of 10000 is {} \".format(step, mean))\n",
    "            print(\"Actions: {}\".format(np.sum(action_avg, axis = 0)))\n",
    "            list_avg_rwd.append(mean)\n",
    "            list_steps.append(step)\n",
    "            \n",
    "            plt.scatter(list_steps, list_avg_rwd)\n",
    "            plt.show()\n",
    "            \n",
    "            \n",
    "            all_time_reward_avg = all_time_reward / (step+1)\n",
    "            all_time_reward_avg_history.append(all_time_reward_avg)\n",
    "            all_time_reward_steps_history.append(step)\n",
    "            plt.scatter(all_time_reward_steps_history, all_time_reward_avg_history)\n",
    "            plt.show()\n",
    "        Q_vals_new = model.predict(X_new)\n",
    "        \n",
    "        Q_vals[range(n_agents), actions] = rewards + gamma * np.max(Q_vals_new, axis=1)\n",
    "        model.fit(X, Q_vals, epochs=1, verbose=0)\n",
    "        \n",
    "        Q_vals = Q_vals_new\n",
    "        X = X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
